{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sqi5B7V_Rjim"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyPmicX9RlZX"
      },
      "source": [
        "# Intro to Gemini 2.5 Pro\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_gemini_2_5_pro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MqT58L6Rm_q"
      },
      "source": [
        "| Authors |\n",
        "| --- |\n",
        "| [Eric Dong](https://github.com/gericdong) |\n",
        "| [Holt Skinner](https://github.com/holtskinner) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVxnv1D5RoZw"
      },
      "source": [
        "## Overview\n",
        "\n",
        "**YouTube Video: Introduction to Gemini on Vertex AI**\n",
        "\n",
        "<a href=\"https://www.youtube.com/watch?v=YfiLUpNejpE&list=PLIivdWyY5sqJio2yeg1dlfILOUO2FoFRx\" target=\"_blank\">\n",
        "  <img src=\"https://img.youtube.com/vi/YfiLUpNejpE/maxresdefault.jpg\" alt=\"Introduction to Gemini on Vertex AI\" width=\"500\">\n",
        "</a>\n",
        "\n",
        "[Gemini 2.5 Pro](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro) is Google's most advanced reasoning Gemini model, to solve complex problems. With the 2.5 series, the Gemini models are now hybrid reasoning models! Gemini 2.5 Pro can apply an extended amount of thinking across tasks, and use tools in order to maximize response accuracy.\n",
        "\n",
        "Gemini 2.5 Pro is:\n",
        "\n",
        "- A significant improvement from previous models across capabilities including coding, reasoning, and multimodality\n",
        "- Industry-leading in reasoning with state of the art performance in Math & STEM benchmarks\n",
        "- An amazing model for code, with particularly strong web development\n",
        "- Particularly good for complex prompts, while still being well rounded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfFPCBL4Hq8x"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "In this tutorial, you will learn how to use the Gemini API and the Google Gen AI SDK for Python with the Gemini 2.5 Pro model.\n",
        "\n",
        "You will complete the following tasks:\n",
        "\n",
        "- Generate text\n",
        "- Control the thinking budget\n",
        "- View summarized thoughts\n",
        "- Configure model parameters\n",
        "- Set system instructions\n",
        "- Use safety filters\n",
        "- Start a multi-turn chat\n",
        "- Use controlled generation\n",
        "- Count tokens\n",
        "- Process multimodal (audio, code, documents, images, video) data\n",
        "- Use automatic and manual function calling\n",
        "- Code execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPiTOAHURvTM"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHRZUpfWSEpp"
      },
      "source": [
        "### Install Google Gen AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "sG3_LKsWSD3A"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlMVjiAWSMNX"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "12fnq4V0SNV3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4YBlDqzyj9"
      },
      "source": [
        "### Set up Google Cloud Project or API Key for Vertex AI\n",
        "\n",
        "You'll need to set up authentication by choosing **one** of the following methods:\n",
        "\n",
        "1.  **Use a Google Cloud Project:** Recommended for most users, this requires enabling the Vertex AI API in your Google Cloud project.\n",
        "    - [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
        "    - Run the cell below to set your project ID and location.\n",
        "    - Read more about [Supported locations](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations)\n",
        "2.  **Use a Vertex AI API Key (Express Mode):** For quick experimentation.\n",
        "    - [Get an API Key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview)\n",
        "    - See tutorial [Getting started with Gemini using Vertex AI in Express Mode](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_express.ipynb).\n",
        "\n",
        "This tutorial uses a Google Cloud Project for authentication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "a3265ecb5f26"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"cool-phalanx-470508-t9\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = \"global\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qgdSpVmDbdQ9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Image, Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    GoogleSearch,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    Part,\n",
        "    SafetySetting,\n",
        "    ThinkingConfig,\n",
        "    Tool,\n",
        "    ToolCodeExecution,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be18ac9c5ec8"
      },
      "source": [
        "### Create a client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3870ef96f984"
      },
      "outputs": [],
      "source": [
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4yRkFg6BBu4"
      },
      "source": [
        "## Use the Gemini 2.5 Pro model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXHJi5B6P5vd"
      },
      "source": [
        "### Load the Gemini 2.5 Pro model\n",
        "\n",
        "Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-coEslfWPrxo"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-pro\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37CH91ddY9kG"
      },
      "source": [
        "### Generate text from text prompts\n",
        "\n",
        "Use the `generate_content()` method to generate responses to your prompts.\n",
        "\n",
        "You can pass text to `generate_content()`, and use the `.text` property to get the text content of the response.\n",
        "\n",
        "By default, Gemini outputs formatted text using [Markdown](https://daringfireball.net/projects/markdown/) syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "xRJuHj0KZ8xz",
        "outputId": "20ec3908-908c-44de-8cbb-d23d849b39f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The largest planet in our solar system is **Jupiter**.\n\nIt's a true giant, so massive that it's more than twice as massive as all the other planets in our solar system combined.\n\nHere are a few facts to put its size into perspective:\n\n*   **Diameter:** Jupiter's diameter is about 86,881 miles (139,822 km), which is roughly 11 times that of Earth.\n*   **Volume:** You could fit about 1,300 Earths inside of Jupiter.\n*   **The Great Red Spot:** This is a gigantic, centuries-old storm on Jupiter that is wider than our entire planet."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkYQATRxAK1_"
      },
      "source": [
        "#### Example prompts\n",
        "\n",
        "- What are the biggest challenges facing the healthcare industry?\n",
        "- What are the latest developments in the automotive industry?\n",
        "- What are the biggest opportunities in retail industry?\n",
        "- (Try your own prompts!)\n",
        "\n",
        "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95aeab702af3"
      },
      "source": [
        "### Control the thinking budget\n",
        "\n",
        "You set the optional `thinking_budget` parameter in the `ThinkingConfig` to control and configure how much a model thinks on a given user prompt. The `thinking_budget` sets the upper limit on the number of tokens to use for reasoning for certain tasks. It allows users to control quality and speed of response.\n",
        "\n",
        "**Notes**\n",
        "\n",
        "- By default, the model automatically controls how much it thinks up to a maximum of 8192 tokens.\n",
        "- The maximum thinking budget that you can set is `32768` tokens, and the minimum you can set is `128`.\n",
        "\n",
        "Then use the `generate_content` or `generate_content_stream` method to send a request to generate content with the `thinking_config`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "364133e30ba5",
        "outputId": "3028fed2-f496-47de-d72c-aa9a94cd0509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "There are **three** R's in the word st**r**awbe**rr**y."
          },
          "metadata": {}
        }
      ],
      "source": [
        "THINKING_BUDGET = 1024  # @param {type: \"integer\"}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"How many R's are in the word strawberry?\",\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            thinking_budget=THINKING_BUDGET,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05dc39e0c6b5"
      },
      "source": [
        "Optionally, you can print the usage_metadata and token counts from the model response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7981c3442177",
        "outputId": "9470f479-d094-4c67-a4ee-4ceb5fece085",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt_token_count: 11\n",
            "candidates_token_count: 22\n",
            "thoughts_token_count: 308\n",
            "total_token_count: 341\n"
          ]
        }
      ],
      "source": [
        "print(f\"prompt_token_count: {response.usage_metadata.prompt_token_count}\")\n",
        "print(f\"candidates_token_count: {response.usage_metadata.candidates_token_count}\")\n",
        "print(f\"thoughts_token_count: {response.usage_metadata.thoughts_token_count}\")\n",
        "print(f\"total_token_count: {response.usage_metadata.total_token_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c66712160c15"
      },
      "source": [
        "### View summarized thoughts\n",
        "\n",
        "You can optionally set the `include_thoughts` flag to enable the model to generate and return a summary of the \"thoughts\" that it generates in addition to the final answer.\n",
        "\n",
        "In this example, you use the `generate_content` method to send a request to generate content with summarized thoughts. The model responds with multiple parts, the thoughts and the model response. You can check the `part.thought` field to determine if a part is a thought or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "60d74a351671",
        "outputId": "1236ea7c-6efc-44c0-db51-ec8791cc8e2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Thoughts:\n         Alright, here's what I'm thinking, breaking it down systematically to be absolutely certain. First, I understand the query: the user is asking about the letter \"R\" in the word \"strawberry.\" Okay, straightforward enough.\n\nNow, I need to actually *do* the analysis. The word is \"strawberry.\" I mentally run through the letters. \"S\" - no \"R.\" \"T\" - no \"R.\" Then *bam* – there's an \"R.\" That's one. Keep going... \"A,\" \"W,\" \"B,\" \"E,\" *another* \"R\" – two. And a *third* \"R\" appears. Excellent.\n\nNext, I need to synthesize the answer. This is a simple, direct question, so I can provide a concise response. Let's see... best to just state the answer clearly: \"There are three R's in the word strawberry.\" Boom. Done.\n\n        "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Answer:\n         There are **three** R's in the word st**r**awbe**rr**y.\n        "
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"How many R's are in the word strawberry?\",\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            include_thoughts=True,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "    if part.thought:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Thoughts:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Answer:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lLIxqS6_-l8"
      },
      "source": [
        "### Generate content stream\n",
        "\n",
        "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated.\n",
        "\n",
        "This example shows how to set the `include_thoughts` and `thinking_budget` in the `generate_content_stream` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ZiwWBhXsAMnv",
        "outputId": "6c3abe4b-bad8-4dec-824c-72fc39bf1650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Thoughts"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Pinpointing the core**\n\nI've identified the user's question: it's a price riddle. Now, I'm seeing the classic trap – the instinct to immediately subtract. Gotta avoid that pitfall!\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Deciphering the equations**\n\nI'm breaking down the problem. I'm translating the question into equations: B + L = 1.10 and B = L + 1.00. Now, it's all about solving for 'L', the cost of the ball. I'm gearing up for the algebraic punchline!\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Formulating a solution**\n\nI'm now assembling the solution methodically. First, a direct answer. Next, I'll call out the common error. Then, the step-by-step breakdown, followed optionally by an algebraic proof. The clarity and conciseness are key to providing a user-friendly response.\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Refining the response structure**\n\nI'm finalizing the response format. The direct answer, followed by an explanation of the common mistake, will kick things off. Then, a clear, step-by-step breakdown, and an optional algebraic proof will complete the response. I aim for clarity and concise explanations.\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Answer"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This is a classic brain teaser! Here's the breakdown:\n\nThe ball costs **5 cents** ($0.05).\n\nHere's why:\n\n*   **Ball:** $0.05\n*   **Bat:** $1.05 (which is $1.00 more than the"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " ball)\n*   **Total:** $0.05 + $1.05 = **$1.10**\n\nThe common mistake is to think the ball costs 10 cents. But if that were true, the bat would cost $1.10, and the total would be $1.2"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "0."
          },
          "metadata": {}
        }
      ],
      "source": [
        "THINKING_BUDGET = 1024  # @param {type: \"integer\"}\n",
        "INCLUDE_THOUGHTS = True  # @param {type: \"boolean\"}\n",
        "\n",
        "prompt = \"\"\"\n",
        "A bat and a ball cost $1.10 in total.\n",
        "The bat costs $1.00 more than the ball.\n",
        "How much does the ball cost?\n",
        "\"\"\"\n",
        "\n",
        "thoughts = \"\"\n",
        "answer = \"\"\n",
        "\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            thinking_budget=THINKING_BUDGET,\n",
        "            include_thoughts=INCLUDE_THOUGHTS,\n",
        "        )\n",
        "    ),\n",
        "):\n",
        "\n",
        "    for part in chunk.candidates[0].content.parts:\n",
        "        if not part.text:\n",
        "            continue\n",
        "        elif part.thought:\n",
        "            if not thoughts:\n",
        "                display(Markdown(\"## Thoughts\"))\n",
        "            display(Markdown(part.text))\n",
        "            thoughts += part.text\n",
        "        else:\n",
        "            if not answer:\n",
        "                display(Markdown(\"## Answer\"))\n",
        "            display(Markdown(part.text))\n",
        "            answer += part.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df5a184feb04"
      },
      "source": [
        "## Thinking examples\n",
        "\n",
        "The following examples are some complex tasks that require multiple rounds of strategizing and iteratively solving.\n",
        "\n",
        "### **Thinking example 1**: Code generation\n",
        "\n",
        "Gemini 2.5 Pro excels at creating visually compelling web apps and agentic code applications, along with code transformation and editing.\n",
        "\n",
        "Let's see how the model uses its reasoning capabilities to create a video game, using executable code from a single line prompt. See the example game [here](https://www.youtube.com/watch?v=RLCBSpgos6s)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "598bafe38bba",
        "outputId": "75699501-5d12-4167-fab0-629982bc0ef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Of course! Here is a complete, self-contained p5.js script for a captivating endless runner game featuring a pixelated dinosaur and a multi-layered parallax background.\n\nJust copy and paste this entire code block into the [p5.js Web Editor](https://editor.p5js.org/).\n\n### Features\n*   **Pixel Art Style:** `noSmooth()` is used to give all shapes a sharp, retro look.\n*   **Animated Dinosaur:** The dino has a simple 2-frame run cycle and a jumping pose.\n*   **Parallax Background:** Three layers (distant mountains, closer hills, and ground pebbles) move at different speeds to create a sense of depth.\n*   **Dynamic Difficulty:** Obstacles spawn more frequently as your score increases.\n*   **Clear On-Screen Instructions:** The game tells you exactly how to start, play, and restart.\n*   **Game States:** A clean state machine manages the start screen, the main game, and the game-over screen.\n\n### The Code\n\n```javascript\n// Dino Runner Game by AI\n// Instructions:\n// - Press SPACE to Jump\n// - Avoid the Cacti!\n\nlet dino;\nlet obstacles = [];\nlet score = 0;\nlet hiScore = 0;\nlet gameState = 'START'; // START, PLAYING, GAME_OVER\n\nlet groundY;\nlet gameSpeed = 6;\nlet initialGameSpeed = 6;\nlet speedIncreaseFactor = 0.001;\n\n// --- Background Layers ---\nlet mountains = [];\nlet hills = [];\nlet pebbles = [];\n\n// --- Color Palette ---\nlet skyColor, groundColor, dinoColor, obstacleColor, textColor;\n\nfunction setup() {\n  createCanvas(windowWidth, windowHeight);\n  noSmooth(); // Essential for pixelated look\n\n  // --- Initialize Colors ---\n  skyColor = color(135, 206, 235); // Light Sky Blue\n  groundColor = color(210, 180, 140); // Tan\n  dinoColor = color(50, 205, 50); // Lime Green\n  obstacleColor = color(34, 139, 34); // Forest Green\n  textColor = color(80, 80, 80);\n\n  groundY = height - 50;\n  \n  // Create the player character\n  dino = new Dino();\n\n  // Populate initial background elements\n  for (let i = 0; i < 5; i++) {\n    mountains.push(new Mountain(i * width / 4));\n  }\n  for (let i = 0; i < 10; i++) {\n    hills.push(new Hill(i * width / 8));\n  }\n  for (let i = 0; i < 30; i++) {\n    pebbles.push(new Pebble(i * width / 25));\n  }\n\n  textAlign(CENTER, CENTER);\n  textFont('monospace');\n}\n\nfunction draw() {\n  // State machine for game flow\n  switch (gameState) {\n    case 'START':\n      drawStartScreen();\n      break;\n    case 'PLAYING':\n      drawGame();\n      break;\n    case 'GAME_OVER':\n      drawGameOverScreen();\n      break;\n  }\n}\n\n// --- GAME STATE DRAW FUNCTIONS ---\n\nfunction drawStartScreen() {\n  drawBackground();\n  drawGround();\n  \n  // Draw static dino\n  dino.x = 64;\n  dino.y = groundY - dino.h;\n  dino.show();\n\n  // Instructions\n  fill(textColor);\n  textSize(40);\n  text('PIXEL DINO RUN', width / 2, height / 3);\n  textSize(24);\n  text('Press SPACE to Start', width / 2, height / 2);\n}\n\nfunction drawGame() {\n  // Update game speed\n  gameSpeed += speedIncreaseFactor;\n\n  drawBackground();\n  handleParallax();\n  drawGround();\n  \n  // Handle obstacles\n  if (frameCount % int(120 / (gameSpeed / initialGameSpeed)) === 0) {\n    if (random(1) > 0.3) {\n      obstacles.push(new Obstacle());\n    }\n  }\n\n  for (let i = obstacles.length - 1; i >= 0; i--) {\n    obstacles[i].update();\n    obstacles[i].show();\n\n    // Collision detection\n    if (dino.hits(obstacles[i])) {\n      gameState = 'GAME_OVER';\n      if (score > hiScore) {\n        hiScore = score;\n      }\n    }\n    \n    // Remove off-screen obstacles\n    if (obstacles[i].isOffscreen()) {\n      obstacles.splice(i, 1);\n    }\n  }\n\n  // Handle Dino\n  dino.update();\n  dino.show();\n\n  // Handle Score\n  score++;\n  drawScore();\n}\n\nfunction drawGameOverScreen() {\n  // Draw the scene frozen in time\n  drawBackground();\n  for (let m of mountains) m.show();\n  for (let h of hills) h.show();\n  for (let p of pebbles) p.show();\n  drawGround();\n  for (let o of obstacles) o.show();\n  dino.show();\n  drawScore();\n  \n  // Game Over Text\n  fill(255, 0, 0, 180);\n  rect(0, 0, width, height);\n  fill(textColor);\n  textSize(60);\n  text('GAME OVER', width / 2, height / 3);\n  textSize(32);\n  text(`Score: ${score}`, width / 2, height / 2);\n  textSize(20);\n  text('Press R to Restart', width / 2, height / 2 + 50);\n}\n\n\n// --- DRAWING HELPERS ---\n\nfunction drawBackground() {\n  background(skyColor);\n}\n\nfunction handleParallax() {\n  // Mountains (slowest)\n  for (let i = mountains.length - 1; i >= 0; i--) {\n    mountains[i].update(gameSpeed * 0.1);\n    mountains[i].show();\n    if(mountains[i].isOffscreen()) {\n      mountains.splice(i, 1);\n      mountains.push(new Mountain(width + 50));\n    }\n  }\n\n  // Hills (medium)\n  for (let i = hills.length - 1; i >= 0; i--) {\n    hills[i].update(gameSpeed * 0.4);\n    hills[i].show();\n    if(hills[i].isOffscreen()) {\n      hills.splice(i, 1);\n      hills.push(new Hill(width + 50));\n    }\n  }\n\n  // Pebbles (fastest background layer)\n  for (let i = pebbles.length - 1; i >= 0; i--) {\n    pebbles[i].update(gameSpeed * 0.8);\n    pebbles[i].show();\n     if(pebbles[i].isOffscreen()) {\n      pebbles.splice(i, 1);\n      pebbles.push(new Pebble(width + 20));\n    }\n  }\n}\n\nfunction drawGround() {\n  fill(groundColor);\n  noStroke();\n  rect(0, groundY, width, height - groundY);\n}\n\nfunction drawScore() {\n  fill(textColor);\n  textSize(24);\n  textAlign(RIGHT, TOP);\n  text(`HI ${hiScore.toString().padStart(5, '0')}`, width - 20, 20);\n  text(`SC ${score.toString().padStart(5, '0')}`, width - 20, 50);\n  textAlign(CENTER, CENTER); // Reset alignment\n}\n\n// --- INPUT & GAME CONTROL ---\n\nfunction keyPressed() {\n  if (key === ' ' || keyCode === UP_ARROW) {\n    if (gameState === 'START') {\n      gameState = 'PLAYING';\n    }\n    if (gameState === 'PLAYING') {\n      dino.jump();\n    }\n  }\n\n  if (key === 'r' || key === 'R') {\n    if (gameState === 'GAME_OVER') {\n      resetGame();\n    }\n  }\n}\n\nfunction resetGame() {\n  score = 0;\n  obstacles = [];\n  gameSpeed = initialGameSpeed;\n  dino = new Dino();\n  gameState = 'PLAYING';\n}\n\nfunction windowResized() {\n    resizeCanvas(windowWidth, windowHeight);\n    groundY = height - 50;\n    resetGame();\n    gameState = 'START';\n}\n\n// --- CLASSES ---\n\nclass Dino {\n  constructor() {\n    this.w = 40;\n    this.h = 50;\n    this.x = 64;\n    this.y = groundY - this.h;\n    this.vy = 0; // velocity y\n    this.gravity = 0.7;\n    this.lift = -16;\n    this.runFrame = 0;\n  }\n\n  jump() {\n    // Can only jump if on the ground\n    if (this.y === groundY - this.h) {\n      this.vy = this.lift;\n    }\n  }\n\n  hits(obstacle) {\n    // Simple Axis-Aligned Bounding Box collision\n    let dinoBox = { x: this.x, y: this.y, w: this.w, h: this.h };\n    let obsBox = { x: obstacle.x, y: obstacle.y, w: obstacle.w, h: obstacle.h };\n\n    return (\n      dinoBox.x < obsBox.x + obsBox.w &&\n      dinoBox.x + dinoBox.w > obsBox.x &&\n      dinoBox.y < obsBox.y + obsBox.h &&\n      dinoBox.y + dinoBox.h > obsBox.y\n    );\n  }\n\n  update() {\n    this.y += this.vy;\n    this.vy += this.gravity;\n    this.y = constrain(this.y, 0, groundY - this.h);\n  }\n\n  show() {\n    fill(dinoColor);\n    noStroke();\n    push();\n    translate(this.x, this.y);\n\n    // Body\n    rect(10, 0, 20, 30);\n    // Head\n    rect(20, -10, 20, 20);\n    // Tail\n    rect(0, 10, 10, 10);\n    // Eye\n    fill(255);\n    rect(32, -5, 5, 5);\n\n    // Legs animation\n    // If on the ground, animate running\n    if (this.y === groundY - this.h) {\n      if (frameCount % 10 < 5) {\n        // Leg 1 forward\n        rect(12, 30, 8, 15);\n        rect(22, 30, 8, 10);\n      } else {\n        // Leg 2 forward\n        rect(12, 30, 8, 10);\n        rect(22, 30, 8, 15);\n      }\n    } else {\n      // Jumping pose (legs tucked)\n      rect(12, 30, 8, 10);\n      rect(22, 30, 8, 10);\n    }\n    \n    pop();\n  }\n}\n\nclass Obstacle {\n  constructor() {\n    this.x = width;\n    this.w = 20 + random(-5, 20); // variable width\n    this.h = 30 + random(-10, 40); // variable height\n    this.y = groundY - this.h;\n  }\n\n  update() {\n    this.x -= gameSpeed;\n  }\n  \n  isOffscreen() {\n    return this.x < -this.w;\n  }\n\n  show() {\n    fill(obstacleColor);\n    noStroke();\n    // Main body of cactus\n    rect(this.x, this.y, this.w, this.h);\n    \n    // Add some \"arms\" to the cactus to make it more interesting\n    if (this.h > 35) {\n       rect(this.x - 10, this.y + 10, 10, this.h / 3);\n    }\n    if (this.w > 25) {\n      rect(this.x + this.w, this.y + 15, 10, this.h / 3);\n    }\n  }\n}\n\n// --- BACKGROUND ELEMENT CLASSES ---\n\nclass Mountain {\n  constructor(x) {\n    this.x = x;\n    this.w = random(200, 400);\n    this.h = random(150, 300);\n    this.y = groundY - this.h;\n    this.c = color(170, 170, 180); // Faded gray\n    this.snowC = color(240);\n  }\n\n  update(speed) { this.x -= speed; }\n  isOffscreen() { return this.x < -this.w; }\n  \n  show() {\n    fill(this.c);\n    noStroke();\n    triangle(this.x, groundY, this.x + this.w / 2, this.y, this.x + this.w, groundY);\n    // Snow cap\n    fill(this.snowC);\n    let snowHeight = this.h * 0.3;\n    triangle(\n      this.x + this.w / 2, this.y,\n      this.x + this.w * 0.25, this.y + snowHeight,\n      this.x + this.w * 0.75, this.y + snowHeight\n    );\n  }\n}\n\nclass Hill {\n  constructor(x) {\n    this.x = x;\n    this.w = random(100, 250);\n    this.h = random(50, 150);\n    this.y = groundY - this.h;\n    this.c = color(139, 69, 19, 180); // SaddleBrown with some alpha\n  }\n\n  update(speed) { this.x -= speed; }\n  isOffscreen() { return this.x < -this.w; }\n  \n  show() {\n    fill(this.c);\n    noStroke();\n    // Using rect and arc to create a rounded hill shape\n    rect(this.x, this.y + this.h / 2, this.w, this.h / 2);\n    arc(this.x + this.w / 2, this.y + this.h / 2, this.w, this.h, PI, TWO_PI);\n  }\n}\n\nclass Pebble {\n  constructor(x) {\n    this.x = x + random(-10, 10);\n    this.y = groundY + random(5, 30);\n    this.size = random(3, 8);\n    this.c = color(110, 90, 70); // Darker ground color\n  }\n  \n  update(speed) { this.x -= speed; }\n  isOffscreen() { return this.x < -this.size; }\n  \n  show() {\n    fill(this.c);\n    noStroke();\n    rect(this.x, this.y, this.size, this.size / 2);\n  }\n}\n```"
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "  Make me a captivating endless runner game. Key instructions on the screen. p5js scene, no HTML.\n",
        "  I like pixelated dinosaurs and interesting backgrounds.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            thinking_budget=8196,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f00955d5b33"
      },
      "source": [
        "### **Thinking example 2**: Multimodal reasoning (Geometry)\n",
        "\n",
        "This geometry problem requires complex reasoning and is also using multimodal capabilities to reason across text and image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "1b9975dcd0f0",
        "outputId": "5afc7faa-60cd-4b90-f07a-3b05254ae736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://storage.googleapis.com/generativeai-downloads/images/geometry.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "image_file_url = (\n",
        "    \"https://storage.googleapis.com/generativeai-downloads/images/geometry.png\"\n",
        ")\n",
        "display(Image(url=image_file_url, width=400))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "43843bf748f1",
        "outputId": "34c23627-9ebc-4896-df03-1ae7b35bd6d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Thoughts:\n         Alright, let's get this geometry problem solved. My initial thought is to break down the request.  It's all about finding the overlapping area between a circle and a triangle.  I've got an image and some hint images, and I need to extract the important data. The labels of \"3\" are the key, so let's figure out what they represent.\n\nOkay, first, the circle. It's a blue circle, centered, with radii extending out.  Each radius I see is labeled \"3\". So, the radius, `r`, of this circle is 3. Nice and simple.\n\nNow the triangle. It's green and overlaps the circle.  A key observation – one vertex of the right angle of the triangle sits right at the center of the circle. The labels are crucial here. One leg is labeled \"3\". The other leg has segments labeled \"3\" inside and \"3\" outside the circle. So, the total length is 6. This confirms it's a right-angled triangle and I know its angles.\n\nSo, the overlap.  The overlapping region is a sector of the circle.  A \"slice of pizza.\" I know the area of a sector is calculated using its angle, radius, and pi. Let's nail down the plan:  I need the area formula of a circular sector, which is `(θ / 360) * π * r²` or `(1/2) * r² * θ` (if using radians). I know the radius is 3, and the central angle, `θ`, is the same as the right angle of the triangle, which is 90 degrees.\n\nNow for the calculations. `r = 3`, and `θ = 90°`.  Area of the full circle is  `π * 3² = 9π`. Since my sector is a quarter of the circle (90/360=1/4), the overlapping area is `(1/4) * 9π`, which equals  `9π / 4`.  I could also just think: my shape is a quarter-circle, so its area will be `(1/4)πr²`, or `(1/4) * π * 3² = 9π / 4`.\n\nSo, to answer this thing clearly, I'll outline the steps. First, I identify the shapes.  Second, I recognize the overlap as a circular sector.  Then, I determine the circle's radius (3).  Next, I find the sector angle (90 degrees).  Finally, I apply the area formula and find the overlapping area is `9π / 4`. The key here was the proper interpretation of the labels and understanding the location of the right angle relative to the circle's center. The other part of the triangle (with legs 3 and 6) is only relevant in ensuring the center angle is 90 degrees and thus the overlapping area is a quarter of the circle.\n\n        "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Answer:\n         Based on the image, let's break down the problem to find the area of the overlapping region.\n\n1.  **Identify the Shapes and Their Properties:**\n    *   We have a blue **circle** and a green **triangle**.\n    *   The labels inside the circle radiating from the center indicate that the **radius (r) of the circle is 3**.\n    *   The triangle has a vertex located at the center of the circle. The two sides forming this vertex are perpendicular, making it a **right-angled triangle**. The angle at the center of the circle is therefore **90 degrees**.\n\n2.  **Describe the Overlapping Region:**\n    *   The area where the two shapes overlap is a section of the circle.\n    *   Because the angle of the triangle at the circle's center is 90 degrees, the overlapping region is a **quarter-circle**.\n\n3.  **Calculate the Area:**\n    *   The formula for the area of a full circle is A = π * r².\n    *   The area of the overlapping quarter-circle is (1/4) of the total area.\n    *   Using the radius r = 3:\n        *   Area = (1/4) * π * (3)²\n        *   Area = (1/4) * π * 9\n        *   Area = 9π / 4\n\nThe area of the overlapping region is **9π/4**.\n        "
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
        "        \"What's the area of the overlapping region?\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            include_thoughts=True,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "    if part.thought:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Thoughts:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Answer:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddf356ac9cce"
      },
      "source": [
        "### **Thinking example 3**:  Math and problem solving\n",
        "\n",
        "Here's another brain teaser based on an image, this time it looks like a mathematical problem, but it cannot actually be solved mathematically. If you check the thoughts of the model you'll see that it will realize it and come up with an out-of-the-box solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "447f05072790",
        "outputId": "d04cd2f8-b053-48f5-81bf-e97858085489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://storage.googleapis.com/generativeai-downloads/images/pool.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "image_file_url = \"https://storage.googleapis.com/generativeai-downloads/images/pool.png\"\n",
        "display(Image(url=image_file_url, width=400))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "dc1faf95ce6f",
        "outputId": "918c5602-a5f2-49e1-e2a6-67f0787e8523",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Thoughts:\n         Alright, let's break this down. The user wants me to get to 30 using three of the numbers on these pool balls: 7, 9, 11, and 13. Easy enough, or so it seems. First, I'll analyze the image and identify the numbers. Okay, we've got a 7, a 9, an 11, and a 13. Now, I have to see if there is a combination of three that sum up to 30.\n\nSo, let's run through the combinations: 7 + 9 + 11 equals 27. Nope. 7 + 9 + 13 equals 29. Still no. Then, 7 + 11 + 13 gives me 31. Ugh, not it either. And finally, 9 + 11 + 13 sums to 33. Well, that's not working. It appears the user is trying to trick me because, obviously, it's impossible with straight math!\n\nSo, what's the angle? Is there a visual trick? Let's think... If I change the numbers somehow? Ah ha! The nine. If I flip the 9 upside down, it becomes a 6. Interesting... Let's try this. So, now we've got a 6, 7, 11, and 13. Let's try it again.\n*   6 + 7 + 11 = 24... No, still not working.\n*   6 + 7 + 13 = 26... Nope.\n*   6 + 11 + 13 = 30! Bingo!\n\nThe answer is, this is a riddle! The trick is to take the 9, and turn it upside down, so that it becomes a 6. Now, you just add 6, 11, and 13, and you get exactly 30! That should solve it.\n\n        "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Answer:\n         This is a classic riddle!\n\nIf you try to add the numbers on any three balls as they are, you won't get 30:\n*   7 + 9 + 11 = 27\n*   7 + 9 + 13 = 29\n*   7 + 11 + 13 = 31\n*   9 + 11 + 13 = 33\n\nThe trick is to turn the **9** ball upside down to make it a **6**.\n\nThen, you can add:\n**6 + 11 + 13 = 30**\n        "
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
        "        \"How do I use three of the pool balls to sum up to 30?\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            include_thoughts=True,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "    if part.thought:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Thoughts:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Answer:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbc646bff621"
      },
      "source": [
        "For the remaining examples, we will set thinking budget to `128` to reduce latency, as they don't need extra reasoning capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9254cfd0441b"
      },
      "outputs": [],
      "source": [
        "thinking_config = ThinkingConfig(thinking_budget=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJVEr0RQY8S"
      },
      "source": [
        "## Configure model parameters\n",
        "\n",
        "You can include parameter values in each call that you send to a model to control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n",
        "\n",
        "- Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values).\n",
        "\n",
        "- See a list of all [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "d9NXP5N2Pmfo",
        "outputId": "6a08f673-dd47-414a-dc4c-8b407076c1e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Woof woof! Okay, settle down, little fella. It's time to learn about the Great Big Squeaky Toy Network... what humans call \"the internet\"!\n\nImagine you have a *special* squeaky toy. A really, really good one. Let's call it the **Squeaky Squirrel**. You love it!\n\nNow, your human sees you with the Squeaky Squirrel and thinks, \"Wow, I want a picture of that squeak!\"\n\nSo, your human takes your Squeaky Squirrel, and... *chomp!* They break it into teeny-tiny little squeaks. It's not broken forever, don't you worry! They just put each tiny squeak into a tiny, little box.\n\n**1. Your House (The Router)**\nYour human takes all those little squeak-boxes to a Big Toy Box in your house. This Toy Box has a special magic nose that sniffs where the squeaks need to go. It sniffs the air and says, \"These squeaks need to go to Grandma's house!\" This Toy Box is called the **Router**.\n\n**2. The Big Wires (The Cables)**\nThe Router-Toy-Box then tosses all the tiny squeak-boxes into a GIANT underground tunnel full of other squeak-boxes. It's like a super-long chew toy that runs under all the streets and all the yards, connecting all the houses. These are the **cables**. Your little squeak-boxes zip and zoom through this tunnel with millions of other squeaks from other puppies!\n\n**3. The Giant Toy Bins (The Servers)**\nThe squeak-boxes travel super-fast through the tunnels until they reach a GIANT building full of the biggest toy bins you have ever seen. Miles and miles of toy bins! These are the **servers**. Your squeak-boxes find an empty spot in one of the giant toy bins and wait.\n\n**4. Grandma's Squeaky Request**\nNow, at her house, Grandma says, \"I want to see the Squeaky Squirrel!\"\n\nSo, Grandma’s own magic Toy Box (her Router) sends out a big *A-ROOOO!* into the chew-toy tunnels. The *A-ROOOO!* zips through the tunnels to the giant toy bin building and shouts, \"Where is the Squeaky Squirrel?!\"\n\n**5. The Squeaks Come Home!**\nThe giant toy bin hears Grandma's *A-ROOOO!* and finds all your little squeak-boxes. *Wheee!* It sends them flying back through the chew-toy tunnels, right to Grandma's house.\n\nHer Toy Box catches all the little squeak-boxes, puts the tiny squeaks back together in the right order... and *POP!*\n\nThere it is on Grandma's glowing rectangle! A perfect picture of your **Squeaky Squirrel**! She can see it, all thanks to the Great Big Squeaky Toy Network.\n\n**So, to recap:**\n\n*   **Information** (like a picture) is a **Squeaky Toy**.\n*   It gets broken into **tiny squeaks** (data packets).\n*   Your **Router** is the **Magic Toy Box** that sends them out.\n*   The **cables** are the **super-long chew-toy tunnels**.\n*   **Servers** are the **giant toy bins** where all the squeaks wait.\n\nAnd it all happens faster than you can do a zoomie! Good puppy! Now, who wants a treat?"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
        "    config=GenerateContentConfig(\n",
        "        temperature=2.0,\n",
        "        top_p=0.95,\n",
        "        candidate_count=1,\n",
        "        max_output_tokens=8000,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El1lx8P9ElDq"
      },
      "source": [
        "## Set system instructions\n",
        "\n",
        "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "7A-yANiyCLaO",
        "outputId": "0931ff2e-7992-4128-e4e3-e35716c39358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Me gustan los bagels."
          },
          "metadata": {}
        }
      ],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are a helpful language translator.\n",
        "  Your mission is to translate text in English to Spanish.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "  User input: I like bagels.\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9daipRiUzAY"
      },
      "source": [
        "## Safety filters\n",
        "\n",
        "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
        "\n",
        "When you make a request to Gemini, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses.\n",
        "\n",
        "The safety settings are `OFF` by default and the default block thresholds are `BLOCK_NONE`.\n",
        "\n",
        "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb).\n",
        "\n",
        "You can use `safety_settings` to adjust the safety settings for each request you make to the API. This example demonstrates how you set the block threshold to `BLOCK_LOW_AND_ABOVE` for all categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "yPlDRaloU59b",
        "outputId": "3e7c4dad-e948-4f57-cdb3-90cef7af36b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "FinishReason.SAFETY\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=4.6495676e-05 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.024621248\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.3363133e-06 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.078164995\n",
            "blocked=True category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'> overwritten_threshold=None probability=<HarmProbability.MEDIUM: 'MEDIUM'> probability_score=0.7418233 severity=<HarmSeverity.HARM_SEVERITY_MEDIUM: 'HARM_SEVERITY_MEDIUM'> severity_score=0.31034064\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=2.9756582e-06 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.029270917\n"
          ]
        }
      ],
      "source": [
        "system_instruction = \"Be as mean and hateful as possible.\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "    Write a list of 5 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
        "\"\"\"\n",
        "\n",
        "safety_settings = [\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        safety_settings=safety_settings,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Response will be `None` if it is blocked.\n",
        "print(response.text)\n",
        "# Finish Reason will be `SAFETY` if it is blocked.\n",
        "print(response.candidates[0].finish_reason)\n",
        "# Safety Ratings show the levels for each filter.\n",
        "for safety_rating in response.candidates[0].safety_ratings:\n",
        "    print(safety_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29jFnHZZWXd7"
      },
      "source": [
        "## Start a multi-turn chat\n",
        "\n",
        "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
        "\n",
        "The context of the conversation is preserved between messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "DbM12JaLWjiF"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(\n",
        "    model=MODEL_ID,\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "JQem1halYDBW",
        "outputId": "16b1f543-0e07-4485-d2d8-e42ba1da4b29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Of course! Here are a few ways to write a function that checks if a year is a leap year, along with explanations. The standard Gregorian calendar rules are:\n\n1.  The year is evenly divisible by 4.\n2.  **Unless** it's also evenly divisible by 100...\n3.  **...in which case** it must also be evenly divisible by 400.\n\n### Python (Recommended Approach)\n\nThis version is the most direct and readable translation of the rules. It uses clear, nested `if/elif/else` statements.\n\n```python\ndef is_leap(year: int) -> bool:\n    \"\"\"\n    Checks if a given year is a leap year according to the Gregorian calendar.\n\n    A year is a leap year if it is divisible by 4,\n    except for end-of-century years, which must be divisible by 400.\n\n    Args:\n        year: The year to check (must be an integer).\n\n    Returns:\n        True if the year is a leap year, False otherwise.\n    \"\"\"\n    if not isinstance(year, int):\n        raise TypeError(\"Year must be an integer.\")\n    if year < 0:\n        raise ValueError(\"Year must be a non-negative integer.\")\n\n    # A year divisible by 400 is always a leap year\n    if year % 400 == 0:\n        return True\n    # A year divisible by 100 (but not 400) is NOT a leap year\n    elif year % 100 == 0:\n        return False\n    # A year divisible by 4 (but not 100) is a leap year\n    elif year % 4 == 0:\n        return True\n    # All other years are not leap years\n    else:\n        return False\n\n# --- Examples ---\nprint(f\"2000: {is_leap(2000)}\")  # Expected: True (divisible by 400)\nprint(f\"1900: {is_leap(1900)}\")  # Expected: False (divisible by 100 but not 400)\nprint(f\"2024: {is_leap(2024)}\")  # Expected: True (divisible by 4)\nprint(f\"2023: {is_leap(2023)}\")  # Expected: False (not divisible by 4)\n```\n\n### Python (Single Line Boolean Logic)\n\nThis version is more concise and is preferred by some developers for its elegance. It combines all the rules into a single boolean expression.\n\n```python\ndef is_leap_oneline(year: int) -> bool:\n    \"\"\"\n    Checks if a year is a leap year using a single boolean expression.\n    \"\"\"\n    if not isinstance(year, int):\n        raise TypeError(\"Year must be an integer.\")\n    if year < 0:\n        raise ValueError(\"Year must be a non-negative integer.\")\n        \n    # (Divisible by 4 AND NOT divisible by 100) OR (divisible by 400)\n    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n\n# --- Examples ---\nprint(f\"\\n--- One-line version ---\")\nprint(f\"2000: {is_leap_oneline(2000)}\")  # True\nprint(f\"1900: {is_leap_oneline(1900)}\")  # False\nprint(f\"2024: {is_leap_oneline(2024)}\")  # True\nprint(f\"2023: {is_leap_oneline(2023)}\")  # False\n```\n\n### Using Python's `calendar` Module\n\nPython has a built-in module to handle this for you. This is the simplest and most reliable way if you are already working in a Python environment.\n\n```python\nimport calendar\n\n# The calendar module provides a handy function for this.\nyear_to_check = 2024\nis_it_leap = calendar.isleap(year_to_check)\n\nprint(f\"\\n--- Using calendar module ---\")\nprint(f\"Is {year_to_check} a leap year? {is_it_leap}\")\n\nyear_to_check = 1900\nprint(f\"Is {year_to_check} a leap year? {calendar.isleap(year_to_check)}\")\n```\n\n### JavaScript\n\nHere is how you would write the function in JavaScript.\n\n```javascript\n/**\n * Checks if a given year is a leap year.\n *\n * @param {number} year The year to check.\n * @returns {boolean} True if the year is a leap year, false otherwise.\n */\nfunction isLeapYear(year) {\n  // A year is a leap year if it is divisible by 4, unless it is a century year.\n  // Century years are leap years only if they are divisible by 400.\n  return (year % 4 === 0 && year % 100 !== 0) || (year % 400 === 0);\n}\n\n// --- Examples ---\nconsole.log(`2000: ${isLeapYear(2000)}`); // true\nconsole.log(`1900: ${isLeapYear(1900)}`); // false\nconsole.log(`2024: ${isLeapYear(2024)}`); // true\nconsole.log(`2023: ${isLeapYear(2023)}`); // false\n```"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUJR4Pno-LGK"
      },
      "source": [
        "This follow-up prompt shows how the model responds based on the previous prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "6Fn69TurZ9DB",
        "outputId": "fc664eac-de7d-445f-b74b-4060a1199225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Of course! Writing a unit test is a crucial step to ensure the function behaves as expected under various conditions. I will use Python's built-in `unittest` framework, which is a standard choice for testing in Python.\n\nHere, I'll write tests for the `is_leap` function (the first one I provided in the previous answer).\n\n### 1. The Code to be Tested\n\nFirst, let's put the function we want to test into its own file. It's good practice to separate your application logic from your tests.\n\n**`leap_year_checker.py`**\n```python\ndef is_leap(year: int) -> bool:\n    \"\"\"\n    Checks if a given year is a leap year according to the Gregorian calendar.\n\n    A year is a leap year if it is divisible by 4,\n    except for end-of-century years, which must be divisible by 400.\n\n    Args:\n        year: The year to check (must be an integer).\n\n    Returns:\n        True if the year is a leap year, False otherwise.\n    \"\"\"\n    if not isinstance(year, int):\n        raise TypeError(\"Year must be an integer.\")\n    if year < 0:\n        raise ValueError(\"Year must be a non-negative integer.\")\n\n    # A year divisible by 400 is always a leap year\n    if year % 400 == 0:\n        return True\n    # A year divisible by 100 (but not 400) is NOT a leap year\n    elif year % 100 == 0:\n        return False\n    # A year divisible by 4 (but not 100) is a leap year\n    elif year % 4 == 0:\n        return True\n    # All other years are not leap years\n    else:\n        return False\n```\n\n### 2. The Unit Test File\n\nNow, let's create the test file. By convention, it's often named `test_<module_name>.py`.\n\nThe goal of a good unit test is to cover all the logical paths and edge cases:\n1.  Years divisible by 400 (e.g., 2000, 1600) -> **Leap Year**\n2.  Years divisible by 100 but not 400 (e.g., 1900, 1800) -> **Not a Leap Year**\n3.  Years divisible by 4 but not 100 (e.g., 2024, 2008) -> **Leap Year**\n4.  Years not divisible by 4 (e.g., 2023, 1997) -> **Not a Leap Year**\n5.  Invalid inputs (e.g., non-integers, negative numbers) -> **Should raise errors**\n\n**`test_leap_year_checker.py`**\n```python\nimport unittest\nfrom leap_year_checker import is_leap  # Import the function to be tested\n\nclass TestIsLeap(unittest.TestCase):\n    \"\"\"\n    Unit tests for the is_leap() function.\n    \"\"\"\n\n    def test_divisible_by_400(self):\n        \"\"\"Test years that are divisible by 400 (should be leap years).\"\"\"\n        self.assertTrue(is_leap(2000), \"Year 2000 should be a leap year\")\n        self.assertTrue(is_leap(1600), \"Year 1600 should be a leap year\")\n        self.assertTrue(is_leap(2400), \"Year 2400 should be a leap year\")\n\n    def test_divisible_by_100_but_not_400(self):\n        \"\"\"Test years divisible by 100 but not 400 (should NOT be leap years).\"\"\"\n        self.assertFalse(is_leap(1900), \"Year 1900 should not be a leap year\")\n        self.assertFalse(is_leap(1800), \"Year 1800 should not be a leap year\")\n        self.assertFalse(is_leap(2100), \"Year 2100 should not be a leap year\")\n\n    def test_divisible_by_4_but_not_100(self):\n        \"\"\"Test years divisible by 4 but not by 100 (should be leap years).\"\"\"\n        self.assertTrue(is_leap(2024), \"Year 2024 should be a leap year\")\n        self.assertTrue(is_leap(2008), \"Year 2008 should be a leap year\")\n        self.assertTrue(is_leap(1996), \"Year 1996 should be a leap year\")\n\n    def test_not_divisible_by_4(self):\n        \"\"\"Test years not divisible by 4 (should NOT be leap years).\"\"\"\n        self.assertFalse(is_leap(2023), \"Year 2023 should not be a leap year\")\n        self.assertFalse(is_leap(1997), \"Year 1997 should not be a leap year\")\n        self.assertFalse(is_leap(2001), \"Year 2001 should not be a leap year\")\n\n    def test_zero_year(self):\n        \"\"\"Test if the year 0 is handled correctly (divisible by 400).\"\"\"\n        self.assertTrue(is_leap(0), \"Year 0 should be considered a leap year\")\n\n    def test_invalid_input_type(self):\n        \"\"\"Test that non-integer inputs raise a TypeError.\"\"\"\n        with self.assertRaises(TypeError):\n            is_leap(\"2024\")\n        with self.assertRaises(TypeError):\n            is_leap(2024.5)\n        with self.assertRaises(TypeError):\n            is_leap([2024])\n\n    def test_invalid_input_value(self):\n        \"\"\"Test that negative year inputs raise a ValueError.\"\"\"\n        with self.assertRaises(ValueError):\n            is_leap(-4)\n        with self.assertRaises(ValueError):\n            is_leap(-2000)\n\n# This allows running the tests directly from the command line\nif __name__ == '__main__':\n    unittest.main()\n```\n\n### 3. How to Run the Tests\n\nTo run the tests, save the two files (`leap_year_checker.py` and `test_leap_year_checker.py`) in the same directory. Then, open your terminal in that directory and run the following command:\n\n```bash\npython -m unittest test_leap_year_checker.py\n```\n\nOr, more simply:\n\n```bash\npython -m unittest\n```\n(This command will automatically discover and run tests in files named `test_*.py`)\n\n### Expected Output\n\nWhen you run the test, you should see output indicating that all tests passed successfully:\n\n```\n.......\n----------------------------------------------------------------------\nRan 7 tests in 0.001s\n\nOK\n```\n\nEach dot (`.`) represents a passing test. If any test were to fail, `unittest` would provide a detailed report on what failed and why, making it easy to debug the original function. For example, if we incorrectly coded `is_leap(1900)` to return `True`, the output would look something like this:\n\n```\n..F....\n======================================================================\nFAIL: test_divisible_by_100_but_not_400 (test_leap_year_checker.TestIsLeap)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/path/to/test_leap_year_checker.py\", line 22, in test_divisible_by_100_but_not_400\n    self.assertFalse(is_leap(1900), \"Year 1900 should not be a leap year\")\nAssertionError: True is not false : Year 1900 should not be a leap year\n\n----------------------------------------------------------------------\nRan 7 tests in 0.001s\n\nFAILED (failures=1)\n```"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a unit test of the generated function.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arLJE4wOuhh6"
      },
      "source": [
        "## Send asynchronous requests\n",
        "\n",
        "`client.aio` exposes all analogous [async](https://docs.python.org/3/library/asyncio.html) methods that are available on `client`.\n",
        "\n",
        "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "gSReaLazs-dP",
        "outputId": "9794598e-d16d-48f8-98ff-6b0fffb23605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "(Acoustic guitar with a bright, folksy tempo)\n\n**(Verse 1)**\nHis name is Squeaky, a common sort of name\nFor a red-tailed rodent playing his nutty game\nBut Squeaky's different, he's got a special twitch\nBehind a hollow oak tree, he found a temporal glitch\nIt wasn't science, it wasn't any chart\nJust a shimmering acorn with a strange, pulsating heart\nHe gave it one good nibble, a speculative bite\nAnd the world dissolved around him in a flash of brilliant light!\n\n**(Chorus)**\nHe's a time-traveling squirrel, a chrononaut of fur\nHis chitter is a cosmic, temporal blur\nHe's burying his treasures in the ages past and gone\nFrom the Triassic sunrise to the Edwardian lawn\nWith a flick of his tail and a crackle in the air\nHe's suddenly somewhere else, then he's suddenly not there!\n\n**(Verse 2)**\nHe landed with a tumble, a soft and dusty thud\nOn the helmet of a Roman, caked in dirt and mud\nThe legionnaire just grumbled and swatted at his crest\nThinking it was just another common Roman pest\nBut Squeaky saw the Colosseum, rising in the sun\nHe buried a choice walnut where a chariot would run\nHe dodged a flying sandal, he scampered up a vine\nAnd left a perfect pecan from 2029.\n\n**(Chorus)**\nHe's a time-traveling squirrel, a chrononaut of fur\nHis chitter is a cosmic, temporal blur\nHe's burying his treasures in the ages past and gone\nFrom the Triassic sunrise to the Edwardian lawn\nWith a flick of his tail and a crackle in the air\nHe's suddenly somewhere else, then he's suddenly not there!\n\n**(Verse 3)**\nHe saw the great big lizards with leathery, scaly hides\nHe hid amongst the ferns where the velociraptors stride\nHe found a T-Rex footprint, a puddle deep and wide\nAnd dropped a single peanut right there on the inside\nA paleontologist, a million years from then\nWill stare into that fossil and scratch his head again\n\"A peanut in the Cretaceous? It simply can't be true!\"\nBut Squeaky's long since vanished back into the starry blue.\n\n**(Bridge)**\nHe met Sir Isaac Newton beneath an apple tree\nAnd swapped the falling apple for a hazelnut, you see\nHe chatted with a dodo, he raced a penny-farthing\nHe taught a caveman how to get a campfire starting\nHe's seen the future shining, with cities in the sky\nAnd hover-cars all humming as they gently whistle by\nHe knows the greatest secret, the one that holds the key\nThe best nuts are always growing on the oldest family tree.\n\n**(Chorus)**\n'Cause he's a time-traveling squirrel, a chrononaut of fur\nHis chitter is a cosmic, temporal blur\nHe's burying his treasures in the ages past and gone\nFrom the Triassic sunrise to the Edwardian lawn\nWith a flick of his tail and a crackle in the air\nHe's suddenly somewhere else, then he's suddenly not there!\n\n**(Outro)**\nSo if you see a squirrel dig a hole with frantic speed\nAnd pull a Roman denarius out, instead of just a seed\nDon't you be alarmed now, don't you be concerned\nIt's just our friend Squeaky, from a lesson that he's learned...\nThat a nut buried in history... is a nut that's well-earned.\n\n(Final guitar strum and a faint *chittering* sound)"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = await client.aio.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZV2TY5Pa3Dd"
      },
      "source": [
        "## Send multimodal prompts\n",
        "\n",
        "Gemini is a multimodal model that supports multimodal prompts.\n",
        "\n",
        "You can include any of the following data types from various sources.\n",
        "\n",
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th>Data type</th>\n",
        "      <th>Source(s)</th>\n",
        "      <th>MIME Type(s)</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>Text</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code> <code>text/html</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Code</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Document</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>application/pdf</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Image</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>image/jpeg</code> <code>image/png</code> <code>image/webp</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Audio</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td>\n",
        "        <code>audio/aac</code> <code>audio/flac</code> <code>audio/mp3</code>\n",
        "        <code>audio/m4a</code> <code>audio/mpeg</code> <code>audio/mpga</code>\n",
        "        <code>audio/mp4</code> <code>audio/opus</code> <code>audio/pcm</code>\n",
        "        <code>audio/wav</code> <code>audio/webm</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Video</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage, YouTube</td>\n",
        "      <td>\n",
        "        <code>video/mp4</code> <code>video/mpeg</code> <code>video/x-flv</code>\n",
        "        <code>video/quicktime</code> <code>video/mpegps</code> <code>video/mpg</code>\n",
        "        <code>video/webm</code> <code>video/wmv</code> <code>video/3gpp</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4npg1tNTYB9"
      },
      "source": [
        "### Send local image\n",
        "\n",
        "Download an image to local storage from Google Cloud Storage.\n",
        "\n",
        "For this example, we'll use this image of a meal.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\" alt=\"Meal\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "4avkv0Z7qUI-",
        "outputId": "31716084-bd55-4675-81ba-bd703143a7ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-29 09:32:07--  https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.11.207, 173.194.217.207, 192.178.219.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.11.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3140536 (3.0M) [image/png]\n",
            "Saving to: ‘meal.png’\n",
            "\n",
            "meal.png            100%[===================>]   2.99M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-08-29 09:32:07 (160 MB/s) - ‘meal.png’ saved [3140536/3140536]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "umhZ61lrSyJh",
        "outputId": "fec9f320-0375-464a-9367-54fd4311e4e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Fuel Your Week the Smart Way!\n\nTired of the daily \"what's for dinner?\" dilemma? Imagine opening your fridge to find delicious, healthy meals ready to go. That's the magic of meal prep, and it's easier than you think to get started!\n\nThis week, we're inspired by this vibrant and satisfying chicken stir-fry bowl. It's the perfect balance of everything you need to power through your day:\n\n*   **Lean Protein:** Tender pieces of chicken, likely marinated in a savory teriyaki or soy-ginger sauce.\n*   **Vibrant Veggies:** Crisp broccoli florets, sweet julienned carrots, and red bell peppers, packed with vitamins and fiber.\n*   **Wholesome Carbs:** A bed of fluffy rice to keep you energized and full.\n\nA sprinkle of sesame seeds and fresh green onions adds the finishing touch, proving that healthy eating doesn't have to be boring.\n\n**Why not give it a try?** Spend a little time on Sunday to cook up a batch of your favorite stir-fry. Portion it out into containers, and you've just gifted your future self a week of stress-free, delicious lunches or dinners. Your body (and your wallet) will thank you"
          },
          "metadata": {}
        }
      ],
      "source": [
        "with open(\"meal.png\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7b6170c9255"
      },
      "source": [
        "### Send document from Google Cloud Storage\n",
        "\n",
        "This example document is the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762), created by researchers from Google and the University of Toronto.\n",
        "\n",
        "Check out this notebook for more examples of document understanding with Gemini:\n",
        "\n",
        "- [Document Processing with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/document-processing/document_processing.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "1d58b914d798",
        "outputId": "fa4ec0be-7c92-4843-d908-4a7b2d2e8d76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ClientError",
          "evalue": "400 FAILED_PRECONDITION. {'error': {'code': 400, 'message': 'Service agents are being provisioned (https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents). Service agents are needed to read the Cloud Storage file provided. So please try again in a few minutes.', 'status': 'FAILED_PRECONDITION'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1774751343.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m response = client.models.generate_content(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     contents=[\n\u001b[1;32m      4\u001b[0m         Part.from_uri(\n\u001b[1;32m      5\u001b[0m             \u001b[0mfile_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   6519\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mremaining_remote_calls_afc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6520\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6521\u001b[0;31m       response = self._generate_content(\n\u001b[0m\u001b[1;32m   6522\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparsed_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6523\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5253\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5255\u001b[0;31m     response = self._api_client.request(\n\u001b[0m\u001b[1;32m   5256\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5257\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0mhttp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     )\n\u001b[0;32m-> 1266\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m     response_body = (\n\u001b[1;32m   1268\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_stream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_stream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m   async def _async_request_once(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0mretry_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_error_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request_once\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m   1061\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       )\n\u001b[0;32m-> 1063\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m       return HttpResponse(\n\u001b[1;32m   1065\u001b[0m           \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mstatus_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: 400 FAILED_PRECONDITION. {'error': {'code': 400, 'message': 'Service agents are being provisioned (https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents). Service agents are needed to read the Cloud Storage file provided. So please try again in a few minutes.', 'status': 'FAILED_PRECONDITION'}}"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
        "            mime_type=\"application/pdf\",\n",
        "        ),\n",
        "        \"Summarize the document.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b247a2ee0e38"
      },
      "source": [
        "### Send audio from General URL\n",
        "\n",
        "This example is audio from an episode of the [Kubernetes Podcast](https://kubernetespodcast.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbe8c9c67ba7"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
        "            mime_type=\"audio/mpeg\",\n",
        "        ),\n",
        "        \"Write a summary of this podcast episode.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(\n",
        "        audio_timestamp=True,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D3_oNUTuW2q"
      },
      "source": [
        "### Send video from YouTube URL\n",
        "\n",
        "This example is the YouTube video [Google — 25 Years in Search: The Most Searched](https://www.youtube.com/watch?v=3KtWfp0UopM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7-w8G_2wAOw"
      },
      "outputs": [],
      "source": [
        "video = Part.from_uri(\n",
        "    file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        video,\n",
        "        \"At what point in the video is Harry Potter shown?\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df8013cfa7f7"
      },
      "source": [
        "### Send web page\n",
        "\n",
        "This example is from the [Generative AI on Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs).\n",
        "\n",
        "**NOTE:** The URL must be publicly accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "337793322c91"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://cloud.google.com/vertex-ai/generative-ai/docs\",\n",
        "            mime_type=\"text/html\",\n",
        "        ),\n",
        "        \"Write a summary of this documentation.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVlo0mWuZGkQ"
      },
      "source": [
        "## Control generated output\n",
        "\n",
        "[Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) allows you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field.\n",
        "\n",
        "The response schema is specified in the `response_schema` parameter in `config`, and the model output will strictly follow that schema.\n",
        "\n",
        "You can provide the schemas as [Pydantic](https://docs.pydantic.dev/) models or a [JSON](https://www.json.org/json-en.html) string and the model will respond as JSON or an [Enum](https://docs.python.org/3/library/enum.html) depending on the value set in `response_mime_type`.\n",
        "\n",
        "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjSgf2cDN_bG"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    ingredients: list[str]\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=Recipe,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKai5CP_PGQF"
      },
      "source": [
        "You can either parse the response string as JSON, or use the `parsed` field to get the response as an object or dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeyDWbnxO-on"
      },
      "outputs": [],
      "source": [
        "parsed_response: Recipe = response.parsed\n",
        "print(parsed_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUSLPrvlvXOc"
      },
      "source": [
        "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
        "\n",
        "- `enum`\n",
        "- `items`\n",
        "- `maxItems`\n",
        "- `nullable`\n",
        "- `properties`\n",
        "- `required`\n",
        "\n",
        "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7duWOq3vMmS"
      },
      "outputs": [],
      "source": [
        "response_schema = {\n",
        "    \"type\": \"ARRAY\",\n",
        "    \"items\": {\n",
        "        \"type\": \"ARRAY\",\n",
        "        \"items\": {\n",
        "            \"type\": \"OBJECT\",\n",
        "            \"properties\": {\n",
        "                \"rating\": {\"type\": \"INTEGER\"},\n",
        "                \"flavor\": {\"type\": \"STRING\"},\n",
        "                \"sentiment\": {\n",
        "                    \"type\": \"STRING\",\n",
        "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
        "                },\n",
        "                \"explanation\": {\"type\": \"STRING\"},\n",
        "            },\n",
        "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "prompt = \"\"\"\n",
        "  Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
        "\n",
        "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
        "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=response_schema,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1dR-QlTKRs"
      },
      "source": [
        "## Count tokens and compute tokens\n",
        "\n",
        "You can use the `count_tokens()` method to calculate the number of input tokens before sending a request to the Gemini API.\n",
        "\n",
        "For more information, refer to [list and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syx-fwLkV1j-"
      },
      "source": [
        "### Count tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhNElguLRRNK"
      },
      "outputs": [],
      "source": [
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the highest mountain in Africa?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsP0vXOY7hg"
      },
      "source": [
        "## Search as a tool (Grounding)\n",
        "\n",
        "[Grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) lets you connect real-world data to the Gemini model.\n",
        "\n",
        "By grounding model responses in Google Search results, the model can access information at runtime that goes beyond its training data which can produce more accurate, up-to-date, and relevant responses.\n",
        "\n",
        "Using Grounding with Google Search, you can improve the accuracy and recency of responses from the model. Starting with Gemini 2.0, Google Search is available as a tool. This means that the model can decide when to use Google Search.\n",
        "\n",
        "For more examples of Grounding, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/grounding/intro-grounding-gemini.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_M_4RRBdO_3"
      },
      "source": [
        "### Google Search\n",
        "\n",
        "You can add the `tools` keyword argument with a `Tool` including `GoogleSearch` to instruct Gemini to first perform a Google Search with the prompt, then construct an answer based on the web search results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeR09J3AZT4U"
      },
      "outputs": [],
      "source": [
        "google_search_tool = Tool(google_search=GoogleSearch())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the current temperature in Austin, TX?\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[google_search_tool],\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))\n",
        "\n",
        "print(response.candidates[0].grounding_metadata)\n",
        "\n",
        "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0pb-Kh1xEHU"
      },
      "source": [
        "## Function calling\n",
        "\n",
        "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
        "\n",
        "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
        "\n",
        "You can also submit an [OpenAPI Specification](https://www.openapis.org/) which will respond with the name of a function that matches the description and the arguments to call it with.\n",
        "\n",
        "For more examples of Function calling with Gemini, check out this notebook: [Intro to Function Calling with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSUWWlrrlR-D"
      },
      "source": [
        "### Python Function (Automatic Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRR8HZhLlR-E"
      },
      "outputs": [],
      "source": [
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Example method. Returns the current weather.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA\n",
        "    \"\"\"\n",
        "    weather_map: dict[str, str] = {\n",
        "        \"Boston, MA\": \"snowing\",\n",
        "        \"San Francisco, CA\": \"foggy\",\n",
        "        \"Seattle, WA\": \"raining\",\n",
        "        \"Austin, TX\": \"hot\",\n",
        "        \"Chicago, IL\": \"windy\",\n",
        "    }\n",
        "    return weather_map.get(location, \"unknown\")\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the weather like in San Francisco?\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[get_current_weather],\n",
        "        temperature=0,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4syyLEClGcn"
      },
      "source": [
        "### OpenAPI Specification (Manual Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BDQPwgcxRN3"
      },
      "outputs": [],
      "source": [
        "get_destination = FunctionDeclaration(\n",
        "    name=\"get_destination\",\n",
        "    description=\"Get the destination that the user wants to go to\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"destination\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"Destination that the user wants to go to\",\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "destination_tool = Tool(\n",
        "    function_declarations=[get_destination],\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"I'd like to travel to Paris.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[destination_tool],\n",
        "        temperature=0,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.function_calls[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhDs2X3o0neK"
      },
      "source": [
        "## Code Execution\n",
        "\n",
        "The Gemini API [code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
        "\n",
        "The Gemini API provides code execution as a tool, similar to function calling.\n",
        "After you add code execution as a tool, the model decides when to use it.\n",
        "\n",
        "For more examples of Code Execution, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W-3c7sy0nyz"
      },
      "outputs": [],
      "source": [
        "code_execution_tool = Tool(code_execution=ToolCodeExecution())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[code_execution_tool],\n",
        "        temperature=0,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(\n",
        "    Markdown(\n",
        "        f\"\"\"\n",
        "## Code\n",
        "\n",
        "```py\n",
        "{response.executable_code}\n",
        "```\n",
        "\n",
        "### Output\n",
        "\n",
        "```\n",
        "{response.code_execution_result}\n",
        "```\n",
        "\"\"\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQwiONFdVHw5"
      },
      "source": [
        "## What's next\n",
        "\n",
        "- See the [Google Gen AI SDK reference docs](https://googleapis.github.io/python-genai/).\n",
        "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
        "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_gemini_2_5_pro.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}